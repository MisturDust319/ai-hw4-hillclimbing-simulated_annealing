{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# AI Homework 4: Hill-Climbing and Simulated Annealing\nStan Slupecki\n\n## Intro\nThis project explores the use of two search algorithms: Hill-Climbing and Simulated Annealing.\n\nHill climbing is a greedy algorithm. It is a simplified search that assumes by always choosing the best solution in the local problem state, it will reach the best global solution. It is quite capable of producing usable results, but it can get \"stuck\" if the local maxima don't steadily lead to the global maximum.\n\nSimulated annealing builds on hill climbing, and tries to fix this problem. Like hill climbing, simulated annealing will choose the best local solution to a problem. However, simulated annealing can also randomly choose a successor. Whether it chooses a random successor is determined by a cooling function. This tweakable function allows the search to choose a more risky solution earlier or later in the search, depending on the needs of the problem. This random selection process allows simulated annealing to continue searching where hill climbing would get stuck. However, this requires the programmer to find a cooling function that produces satsifactory results.\n\nBoth hill climbing and simulated annealing work well with certain types of problem. These functions typically require a constant amount of space, since most implementations only track the current state, not how it got there. As such, these work best for problems where the solution matters more than the steps taken to reach it.\n\nFor this particular project, these functions were used to solve a 8 block slider puzzle.\n\n## Discussion of Solution\n\nFor this project, I used a combination of AIMA code and my own code from the previous project, which also used a slider puzzle.\n\nTo use the AIMA code, I was able to use the classes I wrote for the previous project. However, I had to overwrite the result() method of the Problem class. This method is an objective function, a function used to assign a value to a state in hill climbing and similar search algorithms. Since this changes based on the objective function, I implemented two child classes, one that used a varient of Manhattan Distance, and one that used Displaced Tiles.\n\n## Testing Design\n\nFor this experiment, there were two categories of tests performed.\n\nThe first type of test was designed to demonstrate how effective these search algorithms are. In each search test, the search function was run 30 times (though this could be tweaked). During this step, the number of times it solved the puzzle, the average success of this test, and the average value of the search's objective function were recorded. The search functions tested in this section are:\n\n- hillclimbing with displaced tiles as the objective function\n- hillclimbing with manhattan distance for the objective function\n- simulated annealing with displaced tiles objective function and negative exponential cooling function (further divided into max temperatures of 1, 5, and 20)\n- simulated annealing with manhattan distance objective function and negative exponential cooling function (further divided into max temperatures of 1, 5, and 20)\n- and simulated annealing with manhattan distance objective function and randomly decreasing cooling function (further divided into max temperatures of 1, 5, and 20)\n\nThe second test category was more of a demonstration, intended to show how these problems perform when given a simple puzzle configuration, where only one slider tile is out of place.\n\n## Expected Results\n\nFor these tests, I have some general ideas of how these different functions will perform.\n\nI expect that functions that use manhattan distance for the objective function will outperform those using displaced tiles. This was the case for the previous project, where an a\\* search using manhattan distance vastly outperformed one using displaced tiles.\n\nBecause simulated annealing is basically a varient of hill climbing with a slightly better ability to avoid getting stuck, I expect it to slightly outperform hill climbing.\n\nFor the group of demonstrations with a simple puzzle, I expect both to solve the puzzle.\n\n## Results\n### Setup"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# SETUP\n# this allows the python files in the project directory to be imported\nimport sys\nsys.path.append('/home/nbuser/library')\nimport random\n\nimport eight_puzzle\nfrom search import hill_climbing, simulated_annealing, simulated_annealing_full, exp_schedule",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# CONSTANTS\n\n# PROBLEM MODELS\n# displaced tiles\nPROBLEM_DISPLACED_TILES = eight_puzzle.EightPuzzleProblemDisplacedTiles()\n# manhattan distance\nPROBLEM_MANHATTAN_DISTANCE = eight_puzzle.EightPuzzleProblemManhattanDistance()\n\n# CONSTANT VALUES\n# The number of times to try solve the problem\nNUMBER_OF_TRIES = 30\n\n# Define function that calculates and outputs results\ndef calculate_results(problem, search, iterations=NUMBER_OF_TRIES):\n    # number of successes\n    successes = 0\n    # average successes\n    avg_success = 0\n    # avg heuristic result\n    avg_objective = 0\n\n    for i in range(iterations):\n        result = search(problem)\n        avg_objective += problem.value(result)\n        if eight_puzzle.goal_test(result):\n            successes += 1\n\n    avg_success = successes / iterations\n    avg_objective /= iterations\n    print(\"Number of Tries: \" + str(iterations))\n    print(\"Number of Successes: \" + str(successes))\n    print(\"Average Successes: \" + str(avg_success))\n    print(\"Average Objective Value: \" + str(avg_objective))\n\n# defines a simulated annealing search function for problem\n# with a given k\ndef sa_problem(k):\n    schedule = exp_schedule(k)\n    return lambda problem: simulated_annealing(problem, schedule)\n\n# define a random schedule method, and several random schedules\ndef random_schedule(k, limit=100):\n    return lambda t: (random.random()*(k-t))\n\n# use this to generate a SA search w/ a random schedule\ndef sa_random(k):\n    schedule = random_schedule(k)\n    return lambda problem: simulated_annealing(problem, schedule)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Hill Climbing with Displaced Tiles Heuristic"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# calculate the results for the displaced tiles hill climbing problem\ncalculate_results(PROBLEM_DISPLACED_TILES, hill_climbing)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Hill Climbing with Manhattan Distance"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# calculate the results for the manhattan distance hill climbing problem\ncalculate_results(PROBLEM_MANHATTAN_DISTANCE, hill_climbing)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Simulated Annealing with Displaced Tiles and Negative Exponential Schedule"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# define three SA problems with 1, 5, and 20 as the k value\nsa_1 = sa_problem(1)\nsa_5 = sa_problem(5)\nsa_20 = sa_problem(20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"SA Search, Displaced Tiles, k=1\")\ncalculate_results(PROBLEM_DISPLACED_TILES, sa_1)\nprint(\"SA Search, Displaced Tiles, k=5\")\ncalculate_results(PROBLEM_DISPLACED_TILES, sa_5)\nprint(\"SA Search, Displaced Tiles, k=20\")\ncalculate_results(PROBLEM_DISPLACED_TILES, sa_20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Simulated Annealing with Manhattan Distance and Negative Exponential Schedule"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"SA Search, Manhattan Distance, k=1\")\ncalculate_results(PROBLEM_MANHATTAN_DISTANCE, sa_1)\nprint(\"SA Search, Manhattan Distance, k=5\")\ncalculate_results(PROBLEM_MANHATTAN_DISTANCE, sa_5)\nprint(\"SA Search, Manhattan Distance, k=20\")\ncalculate_results(PROBLEM_MANHATTAN_DISTANCE, sa_20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Simulated Annealing with Manhattan Distance and Random Schedule"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# define three SA problems with 1, 5, and 20 as the k value\n# but w/ a random schedule\nsar_1 = sa_problem(1)\nsar_5 = sa_problem(5)\nsar_20 = sa_problem(20)\n\nprint(\"SA Search w/ Random Schedule, Manhattan Distance, k=1\")\ncalculate_results(PROBLEM_MANHATTAN_DISTANCE, sar_1)\nprint(\"SA Search w/ Random Schedule, Manhattan Distance, k=5\")\ncalculate_results(PROBLEM_MANHATTAN_DISTANCE, sar_5)\nprint(\"SA Search w/ Random Schedule, Manhattan Distance, k=20\")\ncalculate_results(PROBLEM_MANHATTAN_DISTANCE, sar_20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Working With a Simple Puzzle\n\nThis section is meant to demonstrate what happens when these algorithms are used on a puzzle with one displaced tile, as opposed to a random puzzle."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Demnonstrate that these algorithms can solve an Eight Puzzle\")\nprint(\"Hill Climbing, Displaced Tiles\")\nprint(hill_climbing(eight_puzzle.EightPuzzleProblemDisplacedTiles(True)))\nprint(\"Hill Climbing, Manhattan Distance\")\nprint(hill_climbing(eight_puzzle.EightPuzzleProblemManhattanDistance(True)))\nprint(\"Simulated Annealing, Displaced Tiles\")\nprint(simulated_annealing(eight_puzzle.EightPuzzleProblemDisplacedTiles(True)))\nprint(\"Simulated Annealing, Manhattan Distance\")\nprint(simulated_annealing(eight_puzzle.EightPuzzleProblemManhattanDistance(True)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Discussion of Results\n\nThe most obvious observation is that, except for the simple puzzles being solved by the hill climbing algorithms, none of these searches managed to solve the slider puzzle.\n\nHowever, it does seem that my previous observation that simulated annealing generally outperformed hill climbing. The objective functions of used in these tests return higher values when closer to a solution, and the simulated annealing tests typically returned higher average values for their objective functions when compared to the corresponding hill climbing algorithm.\n\nIn addition, the attempt at simulated annealing to solve the simple problem demonstrates the importance of the scheduling function. Because the schedule was too high for a state so close to the solution, it overestimated it's need to keep chosing random successors, and overshot the solution.\n\n### Questions\n\n#### What would a good objective function for this problem be? Explain what you mean by \"good\"\n\nA good objective function would be one that approaches a specific value as the problem approaches the optimal solution. Hill climbing works by taking the problem state, and creating a list of possible solutions that can be reached from the current state. An objective assigns each state a value, and if there is a state with a higher value, the search continues using that state as the reference state. Otherwise, the search ends and returns the current state. It can search for the lowest value, but the AIMA code being used searches for the highest value. Though this doesn’t guarantee the best possible state will be found, it usually produces an adequate solution, and is quite capable of finding the optimal solution.\n\nA good objective function can be derived from the heuristic functions used to solve the eight puzzle with a* search. Two common a* heuristics are counting displaced tiles, and calculating the Manhattan distance between a tile and it’s intended position. Both produce smaller values as they approach the proper solution. So, to use them with the AIMA hill climbing algorithm, you can simply return the reciprocal of their output. As long as care is taken when these heuristic functions return 0 (for both, this signals the current state is the solution) and avoid division by zero, the objective function will output between 0 and 1, or default to 2 if the goal state is found.\n    \nBased on my experience from the previous project, searching using manhattan distance as a heuristic outperforms displaced tiles. As such, I expect similar results when using it as an objective function. \n\n#### (HC1) How well does hill-climbing perform for this task with either of the objective functions; does a combination of them improve its performance?\n\nAs is, with a totally random board, both objective functions fail to solve the puzzle when used in a hill climbing search, even with repeated tests. Tests with a simple board configuration (with only one tile out of place) show that they can be solved with hill climbing. However, the more complicated the starting solution, the more likely the hill climbing search will stop before reaching the goal state.\n\nIt is quite possible to combine objective functions. In fact, it is both easily done and recommended in certain circumstances. However, in this case, the combined objective function of displaced tiles and Manhattan distance would mostly end up being dominated by the results of the Manhattan distance. Combining objective functions is done by choosing the max value returned by calling all objective functions with the same data. Manhattan distance generally returns a higher value. As such, it would mostly be returning the same values as the Manhattan Distance.\n\n#### (HC2) Is hill climbing guaranteed to locate the global maximum/minimum of your objective function? \n\nHill climbing isn't guaranteed to find the global max/min. Hill climbing assumes that following the local max/min will lead to the global maximum or minimum. Depending on the problem, this natural incline towards the solution might not actually occur. Following local max/min values may actually lead to states with the same objective value or a value that trends in the opposite direction, which would halt the search.\n    \nIn particular, in a slider puzzle problem, common objective functions are quite capable of giving the same value to different configurations. This may create a local plateau, halting the search. If this plateau didn't occur at the solution state, this means the solution wasn't found.\n    \n#### On dE in Simulated Annealing\n\nIn a simulated annealing search, the search function will always continue searching along the most optimal path before choosing to follow another path. The AIMA simulated annealing implementation defaults to choosing the first successor who's objective value is larger than the current search node's objective value. If a is the possible successor state, and b is the current state, and a > b, then a-b > 0, where dE = a-b. If dE > 0, this successor node is chosen as the current state, and the process of finding successors is repeated until no longer possible.\n\nBecause of this, the objective function needs to produce larger values for states that are closer to the solution. Otherwise, the search function won't properly identify states that are closer to the solution and will be less likely to find a solution.\n\n#### (SA1) How well does simulated annealing perform for this task with either of the objective functions; does a combination of them improve its performance?\n\nBoth objective functions failed to produce results, even after mutliple tests. This includes using both negative exponential and random cooling functions (each set to different maximum starting values), repeated mutliple times.\n\nAgain, like in hill climbing, it is possible to combine heuristics, but the resulting heuristic would pretty much be dominated by the Manhattan Distance function.\n\n#### (SA2) Is simulated annealing guaranteed to locate the global maximum/minimum of your objective function? \n\nSimulated annealing isn't guaranteed to find a global maximum. Greedy functions like this don't ensure they are choosing optimal solutions, they just assume following local maxima will eventually lead to the global maximum (or likewise for local and global minima). Further, they don't have a way of backtracking if they can no longer make progress. They just stop their search, even if they have not found the optimal solution.\n\nOne difference between simulated annealing and hill climbing is simulated annealing's ability to choose non-climbing solutions. This functionality is controlled by a cooling function, which controls how often a non-climbing solution is accepted. There are many ways to configure this cooling function, and finding a configuration that finds an optimal solution is a matter of tweaking these cooling functions. I suspect my lack of results is more due to a poorly configured cooling funtion than the objective function.\n\n## Conclusion\n\nHill climbing and simulated annealing have limits to how effective they are. In the AIMA textbook, it mentions these functions are best for problems where exact results aren't needed, and the steps taken to reach the final state isn't important. Slider puzzles perhaps aren't the best problem to use hill climbing or simulated annealing search to solve. For slider puzzles, there is a very explicit end state that must be reached, and since we know a proper slider puzzle has a solution, it is rather usefull to know the steps taken to reach the solution.\n\nI think there are two areas where I could've improved my project.\n\nFirst, I used the AIMA hill climbing and simulated annealing search functions. These greedy searches can only seek the highest value. However, the objective functions I knew started high and approached 0 as they approached the solution. This wouldn't work with the AIMA search. In retrospect, the simplest solution would've been to always return the negative of the objective function's output. This would produce a range of values that increasingly approached 0 when a solution was found, which would work with AIMA's greedy searches. My solution of taking the reciprocal of the objective function created an odd schew of values ((0,1] as it approached the solution, and 2.0 for a solution). However, I'm not certain how this may have affected performance.\n\nSecond, for simulated annealing, I think the cooling schedule could've been improved. I honestly didn't know how to go about improving results, besides guessing and checking with different cooling schedules. I imagine if I had a stronger background in statistics, I might have been able to make better sense of the problem, and find a more satisfying solution.\n\nThat being said, these sort of results are not out of place for greedy algorithms. While researching how to approach this project, I found [a very thorough experiment on how to solve a 35-piece slider puzzle with simulated annealing](http://sequoia.ict.pwr.wroc.pl/~witold/aiarr/2009_projekty/35Puzzle/). This experiment, which went into greater depth, testing more objective functions and cooling schedules. Part of the findings of this experiment was the conclusion that, even with the best objective function (euclidean distance) and cooling schedule (random decay), a simulated annealing search isn't guaranteed to find results. As such, I think it's safe to conclude that less than optimal solutions are normal for this type of problem."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}